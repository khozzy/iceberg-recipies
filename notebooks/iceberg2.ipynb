{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_REGION=us-east-1\n",
      "AWS_JAVA_SDK_VERSION=1.12.262\n",
      "AWS_SECRET_ACCESS_KEY=minioadmin\n",
      "AWS_ACCESS_KEY_ID=minioadmin\n"
     ]
    }
   ],
   "source": [
    "!env | grep AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.extensions    org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "\n",
      "spark.sql.defaultCatalog                 icebergcat\n",
      "spark.sql.catalog.icebergcat             org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.catalog.icebergcat.type        hadoop\n",
      "spark.sql.catalog.icebergcat.warehouse   s3a://warehouse\n",
      "spark.sql.catalog.icebergcat.s3.endpoint http://minio:9000\n",
      "spark.sql.catalog.icebergcat.io-impl     org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalogImplementation          in-memory\n",
      "\n",
      "spark.hadoop.fs.s3a.endpoint http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "!cat $SPARK_HOME/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-java-sdk-bundle-1.12.262.jar\n",
      "hadoop-aws-3.3.4.jar\n",
      "iceberg-aws-bundle-1.4.3.jar\n"
     ]
    }
   ],
   "source": [
    "!ls $SPARK_HOME/jars | grep aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "Hadoop version = 3.3.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark.stop()\n",
    "spark = SparkSession.builder.appName(\"iceberg2\").getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(f\"Hadoop version = {spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# conf = spark.sparkContext.getConf()\n",
    "\n",
    "# print(conf.get(\"spark.sql.catalog.icebergcat\"))\n",
    "# pprint(conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# catalog: icebergcat\n",
    "# namespace: default (db?)\n",
    "\n",
    "queries = [\n",
    "    \"SHOW CURRENT NAMESPACE\",\n",
    "    \"create database db\",\n",
    "    \"create table db.t1 (id bigint, data string) USING iceberg\",\n",
    "    \"INSERT INTO db.t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\",\n",
    "    \"select * from db.t1\"\n",
    "]\n",
    "\n",
    "\n",
    "spark.sql(queries[3]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
